%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{color,soul}
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\def\includegraphic{}
%\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ChromeCRISPR - A High Efficacy Hybrid Machine Learning Model for CRISPR/Cas On-Target Predictions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},
   noteref={n1},
   % email={amir\_dp@sfu.ca},
]{\inits{AD}\fnm{Amirhossein} \snm{Daneshpajouh}}
\author[
   addressref={aff1},
   noteref={n1},
   % email={megan\_fowler\_2@sfu.ca},
]{\inits{MF}\fnm{Megan} \snm{Fowler}}
\author[
    corref={aff1},
   addressref={aff1},
   email={wiese@sfu.ca},
]{\inits{KCW}\fnm{Kay C.} \snm{Wiese}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{School of Computing Science, Simon Fraser University}, % university, etc
  \street{8888 University Dr W},                     %
  %\postcode{}                                % post or zip code
  \city{Burnaby},                              % city
  \cny{Canada}                                    % country
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
\note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
%\parttitle{First part title} %if any

% REWRITTEN
% Genome editing has the potential to treat genetic disorders at the source. This can be achieved by modifying the defective DNA through the intentional insertion, deletion, or substitution of genomic content. Among all genome editing technologies, CRISPR/Cas (Clustered Regularly Interspaced Short Palindromic Repeats and CRISPR-associated protein) reigns supreme. CRISPR/Cas uses a single guide RNA (sgRNA) to direct the Cas nuclease to a target DNA region. Due to the ease at creating small RNA molecules, it is possible to have the CRISPR/Cas complex target any arbitrary DNA sequence, thus making it a versatile tool. The efficacy of the complex is dependent on the ability of the sgRNA to bind to a complementary DNA sequence, which varies based on the sequence. Thus, a major challenge is finding sgRNA sequences that have good efficacy. This is where computational models can aid scientists: by predicting the activity of sgRNAs to help narrow the search space of finding the optimal sgRNA. We have used a large new dataset to build and compare the ability of several different machine learning architectures’ ability to predict on-target CRISPR/Cas activity. Additionally, we explored how adding GC content affects our sgRNA activity predictions. We found that our Long Short-Term Memory Network (LSTM) model with the GC content biofeature performed similarly to a state-of-the-art model, DeepHF.

% Updated abstract
\parttitle{Background:} Genome editing has the potential to treat genetic disorders at the source. This can be achieved by modifying the defective DNA through the intentional insertion, deletion, or substitution of genomic content. Among all genome editing technologies, CRISPR/Cas (Clustered Regularly Interspaced Short Palindromic Repeats and CRISPR-associated protein) is considered the gold standard. CRISPR/Cas uses a single guide RNA (sgRNA) to direct the Cas nuclease to a target DNA region. Due to the ease at creating small RNA molecules, it is possible to have the CRISPR/Cas complex target any arbitrary DNA sequence, thus making it a versatile tool. The efficacy of the complex is dependent on the ability of the sgRNA to bind to a complementary DNA sequence, which varies based on the sequence. Thus, a major challenge is finding sgRNA sequences that have good efficacy. This is where computational models can aid scientists: by predicting the activity of sgRNAs to help narrow the search space of finding the optimal sgRNA.
\parttitle{Results:} We have used a large new dataset to build and compare the ability of several different machine learning architectures’ ability to predict on-target CRISPR/Cas activity. Additionally, we explored how adding GC content affects our sgRNA activity predictions. Our novel hybrid models, collectively called ChromeCRISPR, combine the strengths of Convolutional Neural Networks (CNN) with Recurrent Neural Network (RNN) models. They have outperformed state-of-the-art models, including DeepHF and AttCrispr, and our best model, the CNN-GRU hybrid with GC content establishes a new benchmark for predictive accuracy in CRISPR/Cas9 efficacy predictions.
\parttitle{Conclusions:} In summary, we present ChromeCRISPR, a set of hybrid CNN-RNN models designed to predict CRISPR-Cas on-target activity. By combining convolutional and recurrent neural networks, our approach captures both local sequence patterns and long-range dependencies. ChromeCRISPR offers a complementary method to existing tools, enhancing the predictive landscape for CRISPR-Cas genome editing.


%\parttitle{Second part title} %if any
%Text for this section.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{CRISPR-Cas}
\kwd{sgRNA}
\kwd{Genome Editing}
\kwd{Machine Learning}
\kwd{Deep Learning}
\kwd{Hybrid Machine Learning Models}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section*{Introduction}
% REWRITTEN
CRISPR/Cas has revolutionized genome editing, allowing for precise and versatile modifications to an organism's DNA. Originally discovered as the adaptive immune system of bacteria and archaea~\cite{horvath2010crispr, makarova2011evolution}, CRISPR/Cas was repurposed into a powerful genome editing tool\cite{jinek2012programmable}. It has since been applied to several different applications, ranging from therapeutic gene editing, such as treating $\beta$-thalassemia and sickle cell disease \cite{wu2020advances}, to agricultural advancements and creating disease-resistant crops \cite{zhu2020applications}.

The Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) and CRISPR-associated proteins (Cas) complex comprises a single guide RNA (sgRNA) and a nuclease (Cas) \cite{jiang2017crispr}. The sgRNA consists of two components: a stem-loop structure that binds it to the Cas protein and a guide sequence that directs the complex to a complementary DNA region \cite{jiang2017crispr}. When the complex finds a target DNA sequence that is sufficiently complementary, the Cas nuclease induces a double-stranded break \cite{jiang2017crispr}. Following this, the homology-directed repair pathway mends the break and incorporates the intended edit \cite{jiang2017crispr}. This tool allows researchers to target a specific DNA sequence by creating a complementary sgRNA.

While CRISPR/Cas technology has seen widespread use, predicting the activity of the CRISPR/Cas complex has been challenging. For one, it is important to have high efficacy, that is, the proportion of the number of target sites that have incorporated the modification (on-targets) should be high relative to the number of target sites that were unsuccessfully edited. The efficacy is affected by several factors, including sequence and epigenetic features\cite{chari2015unraveling}, as well as secondary structure characteristics\cite{wong2015wu}, thus making it difficult to predict the efficacy of a given sgRNA.

Equally, it is vital that the Cas-sgRNA complex has high specificity. There is some flexibility in how complementary the DNA sequence must be to the sgRNA in order to induce a modification, so it is possible for non-target DNA regions to be edited. These off-target modifications pose a risk to the safety and reliability of CRISPR/Cas genome editing.

Thus, accurate prediction of CRISPR/Cas activity is an essential step in ensuring the continued development of the CRISPR/Cas technology. Assessing the efficacy and specificity of sgRNA activity experimentally is time-consuming and resource-intensive, thus many have turned to computational approaches in order to prioritize biological validation of only the most promising sgRNAs.

Several computational approaches exist for this task, although these tools have their own set of challenges. As mentioned, running CRISPR/Cas activity assays in any great quantity is intractable and due to this, there is little data available to train computational models. This results in models with limited predictive power and restricted size and complexity.

Additionally, the development of computational models for predicting off-target sites is approached as a separate endeavor from on-target predictive models, driven by the necessity to harness distinct datasets. In our recent paper, we explore off-target models, producing novel insights and methodologies \cite{citekey}. Particularly, our research looks at the application of evolutionary algorithms in the domain of off-target prediction.

The work presented here expands upon our previous work in which we assessed the performance of five baseline models: A Random Forest, a CNN (Convolutional Neural Network), and three RNNs (Recurrent Neural Networks) \cite{cibcb}. For one of these RNNs, we also demonstrated how adding GC content could be worthwhile for improving our predictions. Here, we review these baseline models, and then we build upon them to show the effect of adding GC content to the other models, as well as evaluating how the depth of the models affects the performance. We then combine these techniques to create CNN-RNN hybrid models with GC content, which we have collectively named ChromeCRISPR (\textbf{C}NN \textbf{H}ybridized \textbf{R}NN \textbf{O}n-target \textbf{M}odels for Gene \textbf{E}diting with \textbf{CRISPR}). Herein, we show that our best ChromeCRISPR model, a CNN-GRU (Gated Recurrent Unit) hybrid model, outperforms other current state-of-the-art techniques developed from the same dataset. Of these other techniques, DeepHF \cite{wang2019optimized} had previously achieved a Spearman correlation of 0.867 and mean squared error of 0.0094, while AttCRISPR \cite{xiao2021attcrispr} attained a Spearman correlation of 0.872 (mean squared error not reported). Our best ChromeCRISPR model establishes new benchmarks for both Spearman correlation and mean squared error on this dataset, recording 0.876 and 0.0093 respectively.



%The primary objective of this study is to develop a machine learning model for predicting CRISPR/Cas on-target activity by leveraging a recently published dataset and comparing a variety of machine learning architectures. Our goal is to identify which features and techniques result in better on-target predictive accuracy. We hope to contribute a valuable resource for researchers optimizing CRISPR/Cas experiments.

The significance of this study lies in its potential to contribute to the future success of CRISPR/Cas-based therapies for genetic disorders, as well as gaining insights into which machine learning techniques are best suited for this task. Designing highly potent sgRNAs that specifically target the defective DNA will lead to more precise and safe gene editing, thereby facilitating the development of treatments for previously incurable genetic diseases. From the computational perspective, we show the power of hybrid models, as well as the impact of adding GC content as a feature. The establishment of robust predictive models for CRISPR/Cas activity promises a transformative impact on human health and medical advancements.




\section{Background}
% NEW CONTENT
Several tools exist for predicting on-target activity for CRISPR/Cas genome editing. Over the last decade, these tools have seen a great deal of development, beginning as empirical rule-based approaches based on biological experiments \cite{doench2014rational, doench2016optimized, hsu2013dna, chari2015unraveling, moreno2015crisprscan}. These initial tools focused on nucleotide positions and a handful of thermodynamic properties including GC content and the melting temperature.

With the rise in popularity of machine learning, there was a surge in traditional machine learning methods such as regressive and support vector machine (SVM) based methods \cite{doench2016optimized, chari2017sgrna, wang2019crispr,muhammad2020crisprpred}. This was closely followed by more contemporary methods using deep learning and neural networks \cite{chuai2018deepcrispr, wang2019optimized, liu2019prediction, dimauro2019crisprlearner, zhang2020novel, zhang2020c, zhang2021prediction, xiang2021enhancing, ameen2021c}. The deep learning and neural network models were mostly convolutional neural networks (CNNs), however, DeepHF \cite{wang2019optimized} opted to use a recurrent neural network (RNN) to good effect. A detailed account of these tools is given in a recent review by Sherkatghanad et al. \cite{sherkatghanad2023using}.

One of the main challenges faced by all these models is the overall lack of data availability. One of the reasons that it is important to develop predictive models is that it is time-consuming and resource-intensive to test many sgRNAs for their activity, however, this has also resulted in a deficiency in the amount of data available for training large models. Combining datasets across studies is also often unfeasible, since collecting CRISPR datasets involves the compilation of data from diverse cell types and organisms, assessed through various biological assays. This amalgamation results in a heterogeneous dataset and may introduce batch effects, where non-biological factors in an experiment influence the data. Thus to date, models have been limited in their size.

Fortunately, one of the recent on-target predictive tools also conducted a wide-scale experiment for sgRNA on-target activity \cite{wang2019optimized}. This study evaluated the activity of nearly 60,000 sgRNAs across approximately 20,000 genes. This dataset has been used by a few different models, including CRISPRpred(SEQ) \cite{muhammad2020crisprpred}, CRISPR-ONT \cite{zhang2021prediction}, AttCRISPR \cite{xiao2021attcrispr}, and the group who developed the dataset's own tool, DeepHF \cite{wang2019optimized}. However, only DeepHF \cite{wang2019optimized} and AttCRISPR \cite{xiao2021attcrispr} trained and evaluated their models exclusively with this dataset.

In the continuous evolution of CRISPR/Cas9 efficacy prediction, the introduction of AttCRISPR \cite{xiao2021attcrispr} marks a notable advancement, by integrating attention mechanisms, a concept borrowed from the field of natural language processing. It leverages a temporal attention module that dynamically assigns significance to different segments of the sgRNA sequence, enhancing the model's focus on critical nucleotides that influence on-target activity. This mechanism operates through a sophisticated interplay of queries, keys, and values also using bidirectional Gated Recurrent Unit (GRU) networks, enabling a nuanced understanding of sequence determinants. Furthermore, AttCRISPR adopts an ensemble approach, incorporating biologically relevant features such as secondary structure and GC content, processed alongside direct sequence inputs. By combining these elements through a stacking strategy, AttCRISPR achieves a balance of high predictive accuracy and interpretability, making it a valuable tool for sgRNA design and optimization in the CRISPR/Cas9 system.

We sought to build upon these previous works and began exploring the predictive ability of several different machine learning methods in \cite{cibcb}. There, we assessed a Random Forest, a CNN (Convolutional Neural network), a GRU (Gated Recurrent Network), an LSTM (Long Short-Term Memory), and a BiLSTM (Bi-directional LSTM). We discovered that the Random Forest performed significantly worse than the other models, and that the three RNNs significantly outperformed the CNN. We then added GC content as a feature to our LSTM to determine if this may improve its performance at predicting the activity of sgRNAs on either end of the GC content spectrum. These results proved promising, and thus we have expanded upon the idea in this paper.

Here, we evaluate the use of GC content for more of our models, and we also assess the effect of deeper models. Inspired by the success of hybrid CNN-RNN models such as C-RNNCrispr \cite{zhang2020c}, we also experimented with hybrid models in our work. We combined the CNN model with its powerful feature extraction capabilities with various RNNs to create a suite of high performing models, ChromeCRISPR, the best of which outperformed the top models from both DeepHF\cite{wang2019optimized} and AttCRISPR \cite{xiao2021attcrispr}.


\section{Methods}
% In this collection of research,
We employ several different analytical, processing, and predictive technologies. In the following sections we describe our method for analyzing the DeepHF raw data, our processing steps, which include adding features and encoding the biological data into a numerical format, and our general approach for hyperparameter tuning and training. We then discuss the various models that we evaluated followed by the evaluation techniques used.

\subsection{Data Analysis}
The data utilized in this study were sourced from the DeepHF study \cite{wang2019optimized}. This dataset contains activity values for almost 60,000 unique sgRNAs compiled from 20,000 human genes. The study compiled the CRISPR/Cas activity values for three different Cas enzymes: the wildtype SpCas9, eSpCas9, and SpCas9-HF. However, we have elected to only use the wildtype SpCas9 data for our model since it is the standard Cas enzyme and the one most commonly used. Additionally, we elected not to combine the wildtype SpCas9 data with the eSpCas9 and SpCas9-HF data due to the fact that they have statistically significantly different underlying activity distributions, with the mean values being 0.72, 0.35, and 0.48, respectively. Each sgRNA sequence is 20 nucleotides long plus the variable PAM nucleotide, for a total sequence length of 21 nucleotides.

% This paragraph can be rewritten a bit more clearly
% The activity values are given as the indel frequencies. That is, for a given $sgRNA_x$, the activity is the number of successfully edited molecules of $sgRNA_x$ divided by all sequences of $sgRNA_x$ (with and without the edit):
The activity values are measured as indel (insertion or deletion) frequencies, showing the effectiveness of CRISPR/Cas9 editing. For any given sgRNA ($sgRNA_x$) and its corresponding target DNA sequence ($DNA_x$), the activity value shows the fraction of $DNA_x$ target sites that were successfully edited—shown by insertions or deletions—compared to the total $DNA_x$ target sites looked at. To find this ratio, we divide the number of target DNA molecules that have indels, showing successful gene editing, by the total number of target DNA molecules checked, which includes both edited and unedited sequences:
\begin{align*}
    activity_{sgRNA_x} = \dfrac{DNA_x\text{ with edit}}{\text{all }DNA_x \text{ sequences}}
\end{align*}

% This paragraph can be rewritten a bit more clearly
% Thus the activity values range from 0 to 1, where 0 indicates that there was no successfully edited copies of a given sgRNA, and 1 that all copies of a given sgRNA were successfully edited. In our dataset, the mean activity value was 0.73 with a standard deviation of 0.22, thus most sgRNAs had a ratio of edited sequences, as is expected for on-target editing. This does however result in an unbalanced dataset where sgRNAs with low activity are not proportionally represented.

\subsection{Features}
Several biological features have been shown to have an impact on the activity of CRISPR/Cas genome editing including the melting temperatures, free energy, and DNA accessibility \cite{wong2015wu, wang2019prediction, liu2020computational}. For this paper we evaluate only the effect of GC content, however as we discuss later, we plan to assess in detail the effects of these other biological features as well in future work.
\subsubsection{GC Content}
GC content can approximate the thermodynamic stability of an oligonucleotide duplex. Compared to AT/U pairs, GC pairs have an additional hydrogen bond which adds to the stability of the duplex. For CRISPR/Cas experiments, ideal GC content is in the range of 40-60\%, where sgRNAs with GC content outside of that range tend to have lower efficiency \cite{konstantakos2022crispr}. This trend of sgRNAs with extreme GC content having low activity was also noted by Doench et al \cite{doench2014rational} and Wang et al \cite{wang2014genetic}.
The GC content for a given sgRNA ($sgRNA_x$) is calculated as the percentage of guanine and cytosine nucleotides in the sequence:
\begin{align*}
    GC_{sgRNA_x} = \dfrac{Count(G) + Count(C)}{len(sgRNA_x)}
\end{align*}
GC content ranged from 10-95\% with a mean of 52\% and standard deviation of 11\%, thus the majority of our sgRNAs had GC content within the 40-60\% range indicating higher activity. Within our dataset, we also found that GC content within the 40-60\% range was associated with a higher activity (t-test $p<0.05$).

GC content of each sgRNA was added to the models as a single input in the last layer. This ensured that the models were mostly learning the activity based on the sgRNA sequence but still received the additional information from the GC content before making the final prediction.
%\subsubsection{Dissociation Temperature}
%The dissociation temperature is the temperature in degrees Celcius at which 50\% of an oligonucleotide and its complement are in duplex. Similarly to the GC content, this can serve as another approximation to the binding stability of the complex. Using the Wallace Rule \cite{wallace1979hybridization} for short oligonucleotides, the dissociation temperature for a given $sgRNA_x$is given by:
%\begin{align*}
%    Td_{sgRNA_x} = 4(Count(G)+Count(C)) + 2(Count(A) + Count(T))
%\end{align*}
%\subsubsection{Enthalpy, Entropy, and Free Energy}
%Enthalpy, Entropy, and Free Energy are important thermodynamic properties that reflect the internal energy of a system. A previous study has shown that the duplex free energy plays a role in determining sgRNA efficiency with more efficient sgRNAs forming on average less stable structures with a higher free energy \cite{wong2015wu}.
%The nearest neighbor method for estimating thermodynamic properties from nucleic acid sequence was used \cite{santalucia1998unified}:
%\begin{align*}
%    \Delta G(total)
%        & = \Delta G (initiation )\sum_{bigram} \Delta G_{bigram}
%\end{align*}
%where $\Delta G (initiation)$ is the cost for initiating the formation of the duplex. The sum is the sum of all free energies for each bigram in the duplex (e.g. $\Delta G(AA/TT)$). The values used for the bigram energies are given in a paper by Banerjee et al. \cite{banerjee2020improved}. A similar calculation is used for obtaining the enthalpy $\Delta H$ and entropy $\Delta S$.
%\subsubsection{RNA self-folding Free Energy}
%Another thermodynamic property we sought to assess was the self-folding free energy of the sgRNA. RNA secondary structure has been shown to play a role in CRISPR/Cas activity, with more efficient sgRNAs having less of a tendency for self-folding \cite{wong2015wu}. It is thought that when the sgRNA folds in on itself, it is less accessible to initiate DNA binding. The RNA self folding energy was calculated using RNAFold \cite{lorenz2011viennarna}.
%\subsubsection{RNA Stem Loop Formation}
%A specific case of the sgRNA self-folding and resulting in reduced accessibility is in the formation of a stem-loop structure. RNAFold \cite{lorenz2011viennarna} was used to predict the secondary structure of the sgRNA, and whether it contained a predicted stem-loop structure was included as a binary feature.
%There is an additional subsection with the same title: Encoding
\subsection{Encoding and Embedding}
% \subsubsection{Encoding}
One-hot encoding was used to encode the sgRNA sequences into a numerical representation, as it is a common way to translate DNA and RNA sequences into a numerical representation (Figure \ref{fig:Onehot}). Each nucleotide in our 21mer sequence was converted to an array of length 4, thus producing a matrix with dimensions 21x4, where each row is the nucleotide identity and each column represents the position in the sequence. A cell is assigned a value of 1 if at that column position, it contains the row nucleotide, 0 otherwise. This matrix was then flattened to a 1-dimensional array of length 84. For some of the models, depending on the hyperparameter tuning, this was then passed through an embedding layer, resulting in a 1-dimensional array of size 128.

\begin{figure*}[htbp!]
     \centering
     \includegraphics[width=0.95\textwidth]{Figures/onehot.png}
     \caption{One hot encoding of sgRNA 21mer sequences}
     \label{fig:Onehot}
\end{figure*}

%\subsubsection{Embedding}
%For the embedding representation, we used TF-IDF (term frequency - inverse document frequency) on bigrams of our sequence. TF-IDF is a common embedding scheme in natural language processing applications. It represents the combination of two different metrics; the term frequency and the inverse document frequency. The term frequency is the frequency for which a bigram occurs in a given sequence. The inverse document frequency is the frequency for which a bigram occurs across all sequences. We used the TfidfVectorizer from the scikit learn package \cite{scikit-learn}.

%\subsection{Transformations}
%\subsubsection{X Transformations}
%All features were normalized using min-max feature scaling according to the following equation:
%\begin{align*}
%    X^{\prime}=\dfrac {X-X_{\min }}{X_{\max }-X_{\min }}
%\end{align*}
%\subsubsection{y Transformations}
%As noted, the activity values were left-skewed due to the fact that we are using on-target data where we expect the activity to be high for each sgRNA. One method for dealing with skewed data is to transform the activity values to a more normal distribution by using monotonic functions. We assessed the use of several different functions:
%\begin{table}[h!]
%\caption{Transformations for activity values. }
%\begin{tabularx}{\textwidth}{|X|X|}
%    \hline
%        \textbf{Function} & \textbf{Equation} \\ \hline
%        Square & $y^2$ \\ \hline
%        Exponential & $e^y$ \\ \hline
%        Normalize & $\dfrac{y - min(y)}{max(y) - min(y)} $ \\ \hline
%        Boxcox* \cite{box1964analysis} &
%            $\begin{cases}
%                \dfrac{y^{\lambda} - 1}{\lambda} & \lambda \neq 0   \\
%                log(y) & \lambda = 0
%            \end{cases}$ \\ \hline
%        YeoJohnson* \cite{yeo2000new} &
%            $\begin{cases}
%                \dfrac{(y + 1)^{\lambda - 1}}{\lambda}  & y \geq 0, \lambda \neq 0 \\
%                log(y + 1) & y \geq 0, \lambda = 0 \\
%                -\dfrac{(-x + 1)^{2 - \lambda} - 1}{2 - \lambda} & y < 0, \lambda \neq 2 \\
%                -log(-x + 1) & y < 0, \lambda = 2
%            \end{cases}$ \\ \hline
%\end{tabularx}
%\caption*{*$\lambda$ determined by the value that maximizes the log-likelihood function.}
%\label{tab:yTransformations}
%\end{table}

\subsection{Hyperparameter Tuning and Training}
We set aside 15\% of the data for testing and used the remaining 85\% for hyperparameter tuning and training. We used a nested 5-fold cross-validation with Bayesian search for hyperparameter tuning. That is, the 85\% of data originally split off was further divided such that we performed five Bayesian searches for the best hyperparameters on 68\% of the data and used the remaining 17\% for the validation set. The selected hyperparameters were then validated using 5-fold cross-validation. The model was then retrained on the full 85\% portion of the data.

%That is, the 85\% of data originally split off was further divided into 5 equal parts containing 17\% of the original data. For each of these folds, we performed a Bayesian search for the best hyperparameters. This search was then validated using 5-fold cross-validation. The model was then retrained on the full 85\% portion of the data.



Our models were trained on hardware with 4GB RAM memory and utilized 2 CPU cores. Additionally, we utilized NVIDIA V100 Volta GPUs with 32GB HBM2 memory, deployed on the Digital Research Alliance of Canada superclusters. The training process took approximately 20 seconds per iteration on average.


\subsection{Models}

\begin{table}[h!]
\caption{Summary of Models}
\begin{tabularx}{\textwidth}{|X|X|}
    \hline
         \textbf{Model Group} & \textbf{Model Name} \\ \hline
         \multirow{5}{20em}{Base Model}
            & RF \\
            & CNN   \\
            & *GRU \\
            & *LSTM \\
            & *BiLSTM \\ \hline
        \multirow{4}{20em}{Base Model + GC}
            & CNN+GC   \\
            & *GRU+GC \\
            & *LSTM+GC \\
            & *BiLSTM+GC \\ \hline
        \multirow{4}{20em}{Deep Models}
            & deepCNN   \\
            & *deepGRU \\
            & *deepLSTM \\
            & *deepBiLSTM \\ \hline
        \multirow{4}{20em}{Deep Models + GC}
            & deepCNN+GC   \\
            & *deepGRU+GC \\
            & *deepLSTM+GC \\
            & *deepBiLSTM+GC \\ \hline
        \multirow{3}{20em}{ChromeCRISPR (Hybrid CNN Models + GC)}
            & CNN\_GRU+GC \\
            & CNN\_LSTM+GC \\
            & CNN\_BiLSTM+GC \\ \hline
\end{tabularx}
\label{tab:Model_Groups}
\caption*{*Note that the GRU, LSTM, and BiLSTM models are all different types of RNNs}
\end{table}

In this study, we evaluated several machine learning models to predict CRISPR/Cas on-target efficacy. Each model brings a unique approach to handling sequence data, providing insights into their effectiveness for this task. The Random Forest was implemented in scikit learn \cite{scikit-learn}, while the other neural neworks were implemented in PyTorch \cite{paszke2019pytorch}. A summary of the models we evaluated is given in Table \ref{tab:Model_Groups}.
\subsubsection{Random Forest (RF)}
The Random Forest model is an ensemble learning method, utilizing multiple decision trees to make predictions. Its strength lies in its ability to reduce overfitting by averaging the results of individual trees. This model is particularly effective in handling non-linear data and provides a good benchmark for more complex models. In our study, we used a Random Forest as our least complex model and baseline.
The random forest model uses the RandomForestRegressor class from the scikit-learn library \cite{scikit-learn} with \hl{100 estimators}.
\subsubsection{Convolutional Neural Network (CNN)}
Convolutional Neural Networks, widely used in image recognition tasks, have shown promise in sequence analysis due to their ability to detect patterns and motifs in sequential data. In our CNN model, convolutional layers are used to identify and learn sequence features critical for predicting CRISPR/Cas efficacy.
The CNN model consists of two convolutional layers, each with 128 filters, a kernel size of 3, stride of 1, padding of 1, and followed by ReLU activation. The output of the second convolutional layer is then flattened and concatenated. This is then fed into two fully connected layers with batch normalization, the first having \hl{64} units and the second having one output unit. The model uses a sequence embedding, bringing the input size to a 1-dimensional tensor of size 128, and a batch size of \hl{64}.
\subsubsection{Gated Recurrent Unit model (GRU)}
GRU is a type of recurrent neural network that excels in learning from sequential data, such as nucleotide sequences. It is particularly efficient due to its simplified architecture, which helps in faster training without significant compromise in performance, especially in smaller datasets.

\hl{\#TODO}
\subsubsection{Long Short-Term Memory Network (LSTM)}
LSTM, another variant of recurrent neural networks, is known for its ability to capture long-term dependencies in sequence data, an important factor considering the dependencies in nucleotide sequences for CRISPR/Cas targeting efficacy. It addresses the issue of vanishing gradients which is common in traditional RNNs.

\hl{\#TODO}
\subsubsection{Bidirectional Long Short-Term Memory Network (BiLSTM)}
BiLSTM extends the capabilities of standard LSTM by processing the data in both forward and reverse directions. This approach is beneficial for capturing context from both ends of the sequence, potentially improving the model's ability to learn complex patterns in sgRNA sequences.

\hl{\#TODO}
\subsubsection{Deep Models}
\hl{\#TODO}
\subsubsection{ChromeCRISPR}
\hl{\#TODO\\
Why did you choose these models to combine? Why in this order?
How are the models linked? What are the details of the pipeline (inputs, outputs)?}

\subsubsection{Transformer}
Transformer, known for their effectiveness in natural language processing tasks, employ self-attention mechanisms to weigh the significance of different parts of the input data. In the context of sgRNA sequences, this model can help in identifying key nucleotides and motifs that are crucial for targeting efficacy.

\hl{\#TODO}

\subsection{Evaluation Metrics}
To assess the performance of the models, we employed two key metrics: Mean Squared Error (MSE) and Spearman Correlation Coefficient. Scikit-learn's mean squared error \cite{scikit-learn} was used to calculate the MSE and SciPy \cite{2020SciPy-NMeth} was used to calculate the Spearman correlation.
\subsubsection{Mean Squared Error (MSE):} MSE is a widely used metric for regression models. It measures the average of the squares of the errors, i.e., the average squared difference between the estimated values and the actual value. A lower MSE value indicates a better fit of the model to the data. In the context of our study, it quantifies the difference between the predicted efficacy of sgRNAs and their actual efficacy observed in experimental data.
\subsubsection{Spearman Correlation Coefficient:} The Spearman correlation is a non-parametric measure of rank correlation. It assesses how well the relationship between two variables can be described using a monotonic function. In our study, this metric is crucial for understanding the strength and direction of the association between the predicted and observed sgRNA efficacies. A higher Spearman correlation indicates that the model predictions are more closely aligned with the actual efficacy ranks of sgRNAs.

These metrics provide a comprehensive view of the models' performance, considering both the accuracy of predictions (MSE) and the rank-order consistency (Spearman Correlation) with the observed data.

\subsection{Testing:} Our test set was composed of 15\% of the data and included predictions for 8341 sgRNAs. When testing each model, we performed 5-fold cross validation.

\subsection{Statistical Analysis:}
One-way ANOVA \cite{heiman2001understanding} was used to test for significant differences between the means of various groups, such as between models. Following a significant result, Tukey's Honest Significant Difference (HSD) \cite{tukey1949comparing} was used to extract which means were significantly different than the others of that group.

\begin{figure*}[htbp!]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models/MSE_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared errors of five different models}
         \label{fig:Models_MSE}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models/Spearman_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Spearman correlations of five different models}
         \label{fig:Models_Spearman}
     \end{subfigure}

     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models/activity.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different activities}
         \label{fig:Models_Activity}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models/GC_content.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different GC content}
         \label{fig:Models_GC}
     \end{subfigure}
        \caption{Comparison of five different base models. \textbf{a} The mean squared error of the RF, CNN, GRU, LSTM, and BiLSTM models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean MSEs and median MSEs for the models are RF = 0.0197 and 0.0195, CNN = 0.0161 and 0.0156, GRU = 0.0121 and 0.0122, LSTM = 0.0122 and 0.0122, and BiLSTM = 0.0120 and 0.0123, respectively. \textbf{b} The spearman correlation of the RF, CNN, GRU, LSTM, and BiLSTM models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean spearman correlations and median spearman correlations for the models are RF = 0.7550 and 0.7554, CNN = 0.7925 and 0.7902, GRU = 0.8368 and 0.8396, LSTM = 0.8371 and 0.8365, and BiLSTM = 0.8432 and 0.8466, respectively. \textbf{c} The mean squared error of the RF, CNN, GRU, LSTM, and BiLSTM across different deciles of sgRNA activity. \textbf{d} The mean squared error of the RF, CNN, GRU, LSTM, and BiLSTM across sgRNAs with varying GC content.}
        \label{fig:Models}
\end{figure*}

\section{Results}
\subsection{Comparison of Machine Learning Models}
We began by testing out various machine learning model architectures to ascertain if one type performed better than the others (Figure \ref{fig:Models}). We tested a Random Forest, a Convolutional Neural Network (CNN), and three different implementations of a Recurrent Neural Network (RNN): A Gated Recurrent Unit (GRU) model, a Long Short-Term Memory (LSTM) model, and a Bidirectional Long Short-Term Memory (BiLSTM) model. We found that in general, the RNNs performed the best, having both the lowest mean squared error (MSE) (Figure 2a) and highest Spearman correlation (Figure 2b).

As a posthoc analysis, we then sought to inspect these results further to try to elucidate where our models performed well and where they struggled. First, due to the dataset skewing towards higher activity values, we thought that our model may perform better on sgRNAs in this range. We binned the test set by their true activity deciles, that is, the sgRNAs in the bottom 10\% for activity were binned into the $<10\%$ activity bin, and so forth. Our hypothesis proved correct, in that our model struggled to predict the activity of sgRNAs in the bottom 30\% (Figure 2c). We next examined how our model fared on sequences of different GC content. We again binned the sgRNA test set by GC content, however, this time, we did so by the value of the GC content. That is, if a sgRNA had a GC content between 0-10\% it was binned in the $<10\%$ GC bin. In general, we found the predictions to be consistent across GC content, except for the very high ($>90\%$) GC content sgRNAs (Figure 2d). We thought that this could be due to sgRNAs with high GC content having low activity, as then we could attribute this phenomenon to the data imbalance. However, we found that in our dataset, sgRNAs with high GC content were actually associated with high activity (Spearman = 0.3062, $p<0.05$). Thus we can address these two areas of poor predictive performance separately.

\begin{figure*}[htbp!]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models+GC/MSE_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared errors of four different models with GC content}
         \label{fig:Models+GC_MSE}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models+GC/Spearman_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Spearman correlations of four different models with GC content}
         \label{fig:Models+GC_Spearman}
     \end{subfigure}

     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models+GC/activity.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different activities}
         \label{fig:Models+GC_Activity}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Models+GC/GC_content.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different GC content}
         \label{fig:Models+GC_GC}
     \end{subfigure}
        \caption{Comparison of four different base models with GC content added as a feature. \textbf{a} The mean squared error of the CNN+GC, GRU+GC, LSTM+GC, and BiLSTM+GC models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean MSEs and median MSEs for the models are CNN+GC = 0.0170 and 0.0172, GRU+GC = 0.0122 and 0.0123, LSTM+GC = 0.0112 and 0.0113, and BiLSTM+GC = 0.0110 and 0.0111, respectively. \textbf{b} The Spearman correlation of the CNN+GC, GRU+GC, LSTM+GC, and BiLSTM+GC models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean Spearman correlations and median Spearman correlations for the models are CNN+GC = 0.7810 and 0.7807, GRU+GC = 0.8401 and 0.8421, LSTM+GC = 0.8564 and 0.8598, and BiLSTM+GC = 0.8550 and 0.8582, respectively. \textbf{c} The mean squared error of the CNN+GC, GRU+GC, LSTM+GC, and BiLSTM+GC across different deciles of sgRNA activity. \textbf{d} The mean squared error of the CNN+GC, GRU+GC, LSTM+GC, and BiLSTM+GC across sgRNAs with varying GC content.}
        \label{fig:Models+GC}
\end{figure*}

\subsection{Adding GC Content as a Feature}
We decided to focus on improving the predictive performance of our models by addressing the poor performance on sgRNAs with high GC content. To this end, we added the GC content as a feature into our neural network models. We obtained mixed results for this experiment, as the CNN and GRU showed a decrease in performance, with a higher MSE (CNN: 0.0161 to 0.0170, GRU: 0.0121 to 0.0122) and lower Spearman (CNN: 0.7925 to 0.7810, GRU: 0.8368 to 0.8401), however we found that neither of these differences were statistically significant ($p > 0.05$) (Tables \ref{tab:MSE_comparison} \& \ref{tab:Spearman_comparison}). Conversely, the LSTM and BiLSTM both had an increase in performance, with a lower MSE (LSTM: 0.0122 to 0.0112, BiLSTM: 0.0120 to 0.0110), but again, this change was not statistically significant ($p > 0.05$) (Tables \ref{tab:MSE_comparison} \& \ref{tab:Spearman_comparison}).

Comparing our models to one another, we found a similar pattern as was observed previously, with the RNNs significantly outperforming the CNN in the MSE ($p < 0.05$) as well as having higher Spearman correlations (Figures \ref{fig:Models+GC_MSE} \& \ref{fig:Models+GC_Spearman}). Of note, across the three RNN models, less than 0.5\% of the predictions were off by more than 5\%, in contrast with the CNN which was off by more than 5\% for 0.74\% of the predictions.

Similarly to our post hoc analysis of the base models, we binned the sgRNAs by their true activity and by GC content (Figures \ref{fig:Models+GC_Activity} \& \ref{fig:Models+GC_GC}). In this case, the BiLSTM showed the best performance on the sgRNAs with lower activity ($0-30\%$), while the LSTM  had the best performance for both the mid range and highly active sgRNAs (Figure \ref{fig:Models+GC_Activity}). Interestingly, despite performance improving on the low activity, we did not see any statistically significant improvement across any of the GC bins (Figure \ref{fig:Models+GC_GC}).



\subsection{Deeper Models}
% We next sought to assess the effect of deepening the models. Our basic models all had only two specialized layers before a single fully connected layer, thus we evaluated the performance of the models from X-Y layers (Figure ). We found that adding layers ... \hl{AMIR TODO Add figure! Add Models to google drive!}

\begin{figure*}[htbp!]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/DeepLayers/test_loss_vs_layers.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Test Loss (MSE) vs. Number of Layers.}
         \label{fig:test_loss_vs_layers}
     \end{subfigure}%
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/DeepLayers/test_spearman_vs_layers.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Test Spearman Score vs. Number of Layers.}
         \label{fig:test_spearman_vs_layers}
     \end{subfigure}

     \caption{Comparison of test loss (MSE) and test Spearman score vs. number of layers in neural network architectures. The left plot (a) shows how the test loss (MSE) varies with the number of layers. The right plot (b) illustrates the relationship between the test Spearman score and the number of layers. In both plots, it is observed that increasing the number of layers initially improves performance, but beyond a certain point, the performance stabilizes or deteriorates, indicating diminishing returns or potential overfitting.}
     \label{fig:model_performance}
\end{figure*}


Adding more layers to a neural network architecture does not necessarily improve performance beyond a certain point. This phenomenon, often referred to as overfitting, occurs when the model becomes too complex and starts to fit the noise in the training data rather than the underlying patterns. In the case of our experiments, we observe that beyond a certain number of layers, both the test loss and the test Spearman score plateau or even degrade, indicating that the additional complexity does not lead to better generalization performance on unseen data (Figure \ref{fig:model_performance}). Therefore, it is crucial to carefully balance model complexity with performance metrics to avoid overfitting and ensure optimal model performance.

% TODO, might need a review of this paragraph
% We then added the number of layers as a hyperparameter and retrained the models. We found that 3 specialized layers followed by 3 fully connected layers to be an ideal set up for all models. Following this, we tested our models again and found that both the deepCNN and deepGRU had a statistically significant improvement in performance over both the base model and the base model with GC content added ($p<0.05$). We also found statistically significant improvement of the deepLSTM over the base LSTM and of the deepBiLSTM over the base BiLSTM ($p<0.05$).

\begin{figure*}[htbp!]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels/MSE_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared errors of four deep models}
         \label{fig:deepModels_MSE}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels/Spearman_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Spearman correlations of four deep models}
         \label{fig:deepModels_Spearman}
     \end{subfigure}

     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels/activity.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different activities}
         \label{fig:deepModels_Activity}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels/GC_content.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different GC content}
         \label{fig:deepModels_GC}
     \end{subfigure}
        \caption{Comparison of four deep Models. \textbf{a} The mean squared error of the deepCNN, deepGRU, deepLSTM, and deepBiLSTM models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean MSEs and median MSEs for the models are deepCNN = 0.0098 and 0.0098, deepGRU = 0.0099 and 0.0097, deepLSTM = 0.0103 and 0.0103, and deepBiLSTM = 0.0104 and 0.0103, respectively. \textbf{b} The Spearman correlation of the deepCNN, deepGRU, deepLSTM, and deepBiLSTM models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean Spearman correlations and median Spearman correlations for the models are deepCNN = 0.8694 and 0.8740, deepGRU = 0.8684 and 0.8697, deepLSTM = 0.8620 and 0.8641, and deepBiLSTM = 0.8617 and 0.8623, respectively. \textbf{c} The mean squared error of the deepCNN, deepGRU, deepLSTM, and deepBiLSTM across different deciles of sgRNA activity. \textbf{d} The mean squared error of the deepCNN, deepGRU, deepLSTM, and deepBiLSTM across sgRNAs with varying GC content.}
        \label{fig:deepModels}
\end{figure*}


% Updated paragraph
We then added the number of layers as a hyperparameter and retrained the models. Following this, we tested our models again and found that both the deepCNN and deepGRU had a statistically significant improvement in performance over both the base model and the base model with GC content added ($p<0.05$). We also found statistically significant improvement of the deepLSTM over the base LSTM and of the deepBiLSTM over the base BiLSTM ($p<0.05$).

These deeper models all performed similarly, including the CNN which saw a drastic improvement in performance from the base models (Figures \ref{fig:deepModels_MSE} \& \ref{fig:deepModels_Spearman}). The GRU also had a slight edge at predicting sgRNAs in the lower range of activities, though this was not significant ($p>0.05$), but at higher sgRNA activities, the LSTM and CNN significantly outperformed the GRU (Figure \ref{fig:deepModels_Activity}).

Interestingly, the CNN performed statistically significantly better than the base model and base model with GC content on the sgRNAs in the lowest 100 for GC content ($p<0.05$). This trend was not repeated for the RNN models, which did not show any significant changes in the sgRNAs with lower GC content or higher GC content.


\subsection{Deeper Models with GC Content as a Feature}

\begin{figure*}[htbp!]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels+GC/MSE_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared errors of four deep models with GC content}
         \label{fig:deepModels+GC_MSE}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels+GC/Spearman_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Spearman correlations of four deep models with GC content}
         \label{fig:deepModels+GC_Spearman}
     \end{subfigure}

     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels+GC/activity.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different activities}
         \label{fig:deepModels+GC_Activity}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/deepModels+GC/GC_content.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different GC content}
         \label{fig:deepModels+GC_GC}
     \end{subfigure}
        \caption{Comparison of four deep models with GC Content added as a feature. \textbf{a} The mean squared error of the deepCNN+GC, deepGRU+GC, deepLSTM+GC, and deepBiLSTM+GC models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean MSEs and median MSEs for the models are deepCNN+GC = 0.0093 and 0.0092, deepGRU+GC = 0.0098 and 0.0099, deepLSTM+GC = 0.0104 and 0.0106, and deepBiLSTM+GC = 0.0098 and 0.0100, respectively. \textbf{b} The Spearman correlation of the deepCNN+GC, deepGRU+GC, deepLSTM+GC, and deepBiLSTM+GC models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean Spearman correlations and median Spearman correlations for the models are deepCNN+GC = 0.8728 and 0.8756, deepGRU+GC = 0.8668 and 0.8689, deepLSTM+GC = 0.8602 and 0.8623, and deepBiLSTM+GC = 0.8671 and 0.8690, respectively. \textbf{c} The mean squared error of the deepCNN+GC, deepGRU+GC, deepLSTM+GC, and deepBiLSTM+GC across different deciles of sgRNA activity. \textbf{d} The mean squared error of the deepCNN+GC, deepGRU+GC, deepLSTM+GC, and deepBiLSTM+GC across sgRNAs with varying GC content.}
        \label{fig:deepModels+GC}
\end{figure*}

Since we did see some improvement to the base models using GC content, we opted to assess how this would affect our deeper models. Again, we detected no significant difference in the MSE predictions of any of the four models (Figures \ref{fig:deepModels+GC_MSE} \& \ref{fig:deepModels+GC_Spearman}). We also were unable to detect any significant difference in the way each model predicts the activity deciles of the sgRNAs (Figure \ref{fig:deepModels+GC_Activity}) and the models all showed similar performance across sgRNAs with differing amounts of GC content (Figure \ref{fig:deepModels+GC_GC}).

% Kind of think we dont need this figure since nothing really changed when we added GC content


\subsection{ChromeCRISPR Hybrid Models}
Since the CNN was now performing similarly to the RNN models, we thought that combining the CNN with an RNN could allow our model to harness the feature extraction capabilities of the CNN while maintaining the sequence processing abilities of the RNN. We therefore tested hybrid models of a CNN linked to each of the three RNNs (Figure \ref{fig:Hybrid+GC}). We opted to include GC content as a feature once more since despite the RNNs not improving, or even in some cases such as the GRU and LSTM to have reduced performance, the deep CNN did see a slight improvement (Tables \ref{tab:MSE_comparison} \& \ref{tab:Spearman_comparison}).

The CNN\_GRU+GC and the CNN\_BiLSTM+GC models both showed a slight improvement in MSE and Spearman correlation, although the performance of the CNN\_LSTM+GC dipped. None of these performance deviations from their deep model counterparts were found to be statistically significant ($p>0.05$). There was however, a statistically significant difference between the performance of the CNN\_GRU+GC and the CNN\_BiLSTM+GC models with the CNN\_LSTM+GC model (Figures \ref{fig:Hybrid+GC_MSE} \& \ref{fig:Hybrid+GC_Spearman}).

We believe a possible explanation for the under-performance of the CNN\_LSTM+GC model is due to the direction of the sequence in our dataset.
As mentioned in section 2.1, our sgRNA sequence is composed of the 20 nucleotide sequence and the variable PAM nucleotide. This sequence is given in the canonical 5' to 3' direction, and thus the variable PAM nucleotide is the last nucleotide of the sequence. We therefore hypothesize that the reason the CNN\_BiLSTM+GC outperforms the CNN\_LSTM+GC is due to the BiLSTM having access to the PAM nucleotide from the start, whereas the LSTM must traverse the entire sequence before it sees the PAM nucleotide. We believe there could be a similar reason for the CNN\_GRU+GC's out-performance. Since the GRU is less able to retain long-term dependencies, when it arrives at the end of the sequence, it is mostly focusing on the nucleotides at the 3' end, whereas the LSTM is such that is retains information from the 5' end. However, the nucleotides at the 3' end of the sgRNA sequence are known to carry the most influence over the activity of the sgRNA \cite{doench2014rational}, and thus it is possible that in the LSTM nucleotides from the 5' end are impacting the predicted activity more than they ought to. It would be worthwhile to run experiments with the sgRNA sequence reversed such that it runs 3' to 5'.


The CNN\_LSTM+GC model actually showed slightly better performance than the other two in the activity range of 0 to 20\%, then falling far behind for the rest of the activity deciles. The CNN\_GRU+GC and CNN\_BiLSTM+GC models performed quite similarly across all deciles, however, the CNN\_GRU+GC model slightly outperformed the CNN\_BiLSTM+GC model at higher activity deciles (70-100\%) (Figure \ref{fig:Hybrid+GC_Activity}).




\begin{figure*}[htbp!]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Hybrid+GC/MSE_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared errors of three ChromeCRISPR Models}
         \label{fig:Hybrid+GC_MSE}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Hybrid+GC/Spearman_boxplot.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Spearman correlations of three ChromeCRISPR Models}
         \label{fig:Hybrid+GC_Spearman}
     \end{subfigure}

     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Hybrid+GC/activity.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different activities}
         \label{fig:Hybrid+GC_Activity}
     \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Hybrid+GC/GC_content.png}
         \captionsetup{width=0.8\textwidth}
         \caption{Mean squared error of sgRNAs with different GC content}
         \label{fig:Hybrid+GC_GC}
     \end{subfigure}
        \caption{Comparison of three ChromeCRISPR models, each of which is a CNN hybridized RNN model with GC content. \textbf{a} The mean squared error of the CNN\_GRU+GC, CNN\_LSTM+GC, and CNN\_BiLSTM+GC models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean MSEs and median MSEs for the models are CNN\_GRU+GC = 0.0093 and 0.0093, CNN\_LSTM+GC = 0.0115 and 0.0117, and CNN\_BiLSTM+GC = 0.0096 and 0.0097, respectively. \textbf{b} The Spearman correlation of the CNN\_GRU+GC, CNN\_LSTM+GC, and CNN\_BiLSTM+GC models. The horizontal dashed lines represent the means, while the black solid line in each box plot is the median. The mean Spearman correlations and median Spearman correlations for the models are CNN\_GRU+GC = 0.8760 and 0.8796, CNN\_LSTM+GC = 0.8668 and 0.8659, and CNN\_BiLSTM+GC = 0.8700 and 0.8708, respectively. \textbf{c} The mean squared error of the CNN\_GRU+GC, CNN\_LSTM+GC, and CNN\_BiLSTM+GC across different deciles of sgRNA activity. \textbf{d} The mean squared error of the CNN\_GRU+GC, CNN\_LSTM+GC, and CNN\_BiLSTM+GC across sgRNAs with varying GC content.}
        \label{fig:Hybrid+GC}
\end{figure*}

\begin{table}[h!]
\caption{MSE comparison across all models}
\begin{tabularx}{\textwidth}{|X|X|X|X|l|X|}
    \hline
        \textbf{Model Name} & \textbf{Base Model} & \textbf{Model+GC} & \textbf{deepModel} & \textbf{deepModel+GC} & \textbf{*Hybrid+GC}\\ \hline
        CNN & 0.0161 & 0.0170 & 0.0098 & \textbf{0.0093} & --\\ \hline
        GRU & 0.0121 & 0.0122 & 0.0099 & 0.0098 & \textbf{0.0093} \\ \hline
        LSTM & 0.0122 &	0.0112 & 0.0103 & 0.0104 & 0.0115 \\ \hline
        BiLSTM & 0.0120 & 0.0110 & 0.0104 & 0.0098 & 0.0096 \\ \hline
\end{tabularx}
\label{tab:MSE_comparison}
\subcaption*{*Hybrid models are CNN followed by the RNN model. See Table \ref{tab:Model_Groups}.\\
Best results are in bold. For the MSE, the two best models were the deepCNN+GC and the CNN\_GRU+GC.}
\end{table}

\begin{table}[h!]
\caption{Spearman comparison across all models}
\begin{tabularx}{\textwidth}{|X|X|X|X|l|X|}
    \hline
        \textbf{Model Name} & \textbf{Base Model} & \textbf{Model+GC} & \textbf{deepModel} & \textbf{deepModel+GC} & \textbf{*Hybrid+GC}\\ \hline
        CNN & 0.7925 & 0.7810 & 0.8694 & 0.8728 & --\\ \hline
        GRU & 0.8368 & 0.8401 & 0.8684 & 0.8668 & \textbf{0.8760} \\ \hline
        LSTM & 0.8371 &	0.8564 & 0.8620 & 0.8602 & 0.8668 \\ \hline
        BiLSTM & 0.8432 & 0.8550 & 0.8617 & 0.8671 & 0.8700 \\ \hline
\end{tabularx}
\label{tab:Spearman_comparison}
\subcaption*{*Hybrid models are CNN followed by the RNN model. See Table \ref{tab:Model_Groups}.\\
Best results are in bold. For the Spearman correlation, the best performing model was the CNN\_GRU+GC. }
\end{table}


\subsection{Comparison to Other Models}
Two other studies have published results of their models developed on this dataset exclusively: DeepHF and AttCRISPR. Shown in Table \ref{tab:Model_comparison} is a comparison of our ChromeCRISPR models, including our top model, the hybrid CNN\_GRU+GC, with the LSTM models from DeepHF with and without bio-features, as well as the several attention-based models from AttCRISPR with and without bio-features. We found that our CNN\_GRU+GC ChromeCRISPR model outperformed both the base DeepHF and AttCRISPR models, as well as the ones with bio-features.

\begin{table}[h!]
\caption{Comparison to other SotA Models}
\begin{tabularx}{\textwidth}{|X|X|l|l|l|l|}
    \hline
         \textbf{Model Group} & \textbf{Model Name} & \textbf{Spearman Correlation} & \textbf{stdev}  & \textbf{MSE} & \textbf{stdev} \\ \hline
         \multirow{2}{*}{DeepHF} &  RNN & 0.856 & 0.003 & 0.0104 & 0.0003 \\
             & RNN + Bio & 0.867 & 0.002 & 0.0094 & 0.0002 \\ \hline
         \multirow{5}{*}{AttCRISPR} & SpAC & 0.857 & 0.010 & \multicolumn{2}{l|}{\multirow{5}{*}{Not Reported}}\\
          & TAC & 0.862 & 0.010 & \multicolumn{2}{l|}{}\\
          & EnAC & 0.868 & 0.010 & \multicolumn{2}{l|}{}\\
          & EnAC + Bio & 0.868 & 0.003 & \multicolumn{2}{l|}{}\\
          & StAC + Bio & 0.872 & 0.003 & \multicolumn{2}{l|}{}\\ \hline
         \multirow{3}{*}{ChromeCRISPR}
         & CNN\_LSTM+GC & 0.867 & 0.009 & 0.0115 & 0.0009\\
         & CNN\_BiLSTM+GC & 0.870 & 0.009 & 0.0096 & 0.0007\\
         & CNN\_GRU+GC & \textbf{0.876} & 0.008 & \textbf{0.0093} & 0.0006\\
         \hline
\end{tabularx}
\label{tab:Model_comparison}
\captionsetup{justification=justified, singlelinecheck=off}
\subcaption*{Best results are in bold. When compared to other SotA methods used for activity prediction on the DeepHF dataset, our CNN\_GRU+GC model outperformed all DeepHF and AttCRISPR models. }
\end{table}


\section{Discussion}
The strategic incorporation of GC content in the last fully connected layer of our model, just before the final prediction, represents a significant advancement in sgRNA activity prediction. This approach allows the network to initially learn complex representations of sgRNA sequences through multiple layers of abstraction. Integrating GC content at this later stage enables the model to combine learned sequence features with the biological relevance of GC content effectively. This nuanced method showcases the model's ability to leverage both in-depth sequence understanding and critical biological indicators, like GC content, to enhance predictive accuracy.

Our research has been directed towards exploring the efficacy of hybrid models in sgRNA activity prediction, particularly through the integration of convolutional neural networks (CNNs) with recurrent neural network (RNN) architectures. This approach leverages the strengths of both CNNs, with their capability to extract spatial features from sequence data, and RNNs, notably GRU (Gated Recurrent Unit) layers, which excel in capturing temporal dependencies within sequences.

The motivation behind employing a hybrid CNN-GRU architecture stems from the complexity of sgRNA sequences, where both the sequence-specific features and the order of nucleotides play critical roles in determining sgRNA efficacy. CNN layers are adept at identifying relevant patterns across sgRNA sequences, while GRU layers contribute to understanding how these patterns influence activity outcomes based on their arrangement within the sequence.

% I think the Inception model should go in future directions and we shouldnt discuss any results in this paper
%One of our models, CNN\_GRU\_Inception\_GC, incorporates this hybrid architecture while also introducing GC content as a significant biological feature. However, it is important to note that the Inception Module was specifically utilized in this model to enhance feature extraction through its multi-branch convolutional strategy. The primary focus across our hybrid model development has been on the synergistic integration of CNN and RNN architectures to improve predictive performance.

% The decision to incorporate GC content at a later stage in the model, particularly before the final prediction, is based on the premise that combining deep sequence representations with critical genomic features like GC content can offer a more nuanced understanding of sgRNA activity. This method allows the model to leverage learned sequence characteristics in conjunction with established biological indicators, potentially leading to more accurate predictions.

%The CNN\_GRU\_Inception\_GC model demonstrated promising outcomes, signifying a potential improvement over some traditional models with a test loss of 0.009108 and a Spearman correlation of 0.876469, while these results are approached with scientific caution. Overall, the performance highlights the potential of hybrid architectures in sgRNA activity prediction.

In addition to our ChromeCRISPR hybrid models, we also experimented with a standalone Transformer architecture. However, this Transformer-based model did not outperform our deep RNN-based models. This suggests that the specific Transformer architecture we tested might not have been suitable for this task, or its known preference for longer sequences might not align well with the 21-mer sequences we have in this project.

\subsection{Future Directions}
\subsubsection{Adding Attention and Additional Bio-features}
The attention mechanism has shown promise in various sequence-based prediction tasks, allowing models to focus on the most relevant parts of the input sequence \cite{bahdanau2014neural, vaswani2017attention} and could be explored in our model and evaluated against current models such as \cite{choi2016retain}.
CRISPR/Cas9 efficacy is influenced by numerous biological factors beyond the sgRNA sequence itself, such as the target gene's expression level, chromatin accessibility, and the genomic context surrounding the target site \cite{doench2016optimized, sanjana2014improved}. To capture these influences, we intend to incorporate relevant biofeatures extracted from public genomic databases and the literature into our predictive models \cite{uhlen2015tissue, encode2012integrated}.

% Given its success, we plan to explore the integration of an attention layer into our LSTM and BiLSTM architectures for CRISPR/Cas efficacy prediction. This approach is anticipated to enhance model interpretability and performance by dynamically weighing the importance of each nucleotide position, potentially highlighting crucial sequence features that contribute to on-target efficacy. The effectiveness of this method will be evaluated against current models to determine its impact on predictive specificity and accuracy \cite{choi2016retain}.

%\subsection{Adding Biofeatures}
%CRISPR/Cas9 efficacy is influenced by numerous biological factors beyond the sgRNA sequence itself, such as the target gene's expression level, chromatin accessibility, and the genomic context surrounding the target site \cite{doench2016optimized, sanjana2014improved}. To capture these influences, we intend to incorporate relevant biofeatures extracted from public genomic databases and the literature into our predictive models \cite{uhlen2015tissue, encode2012integrated}. This expansion of model inputs is expected to allow the network to learn complex interactions between sgRNA sequences and their biological context, potentially enhancing predictive performance, especially in differentiating between high and low efficacy sgRNAs across various genomic contexts. The impact of these biofeatures on model outcomes will be systematically assessed.

\subsubsection{Embedding for Data Representation}
The representation of sgRNA sequences is critical for accurately capturing the biological information essential for predicting CRISPR/Cas9 efficacy. Traditional one-hot encoding may not capture all relevant sequence characteristics. Therefore, we plan to investigate advanced embedding techniques, such as position-specific scoring matrices (PSSM) and word2vec models, for generating dense vector representations of sgRNA sequences \cite{mikolov2013distributed}. These techniques are expected to provide a more nuanced representation of sgRNA sequences by encapsulating the identity, positional importance, and context of nucleotides within the sequence. The adoption of these advanced embeddings will be explored to uncover latent features correlating with on-target efficacy, with the aim of enhancing model performance and generalizability \cite{alipanahi2015predicting, kim2014convolutional}.

\subsubsection{Generalizability of the Model}
We would like to see how our model can generalize in two different cases: 1) To CRISPR/Cas9 data from other studies, and 2) To other Cas enzymes, e.g. eSpCas9 \cite{slaymaker2016rationally} and SpCas9-HF \cite{kleinstiver2016high}.

%\subsubsection{To CRISPR/Cas9 data from another study}
%Several factors of experimental design can impact the recorded activity of a CRISPR/Cas experiment. These factors include the reagent concentrations, incubation times, and DSB (Double stranded break) detection methods. However, we would still expect that a sgRNA that shows good activity under one set of experimental conditions to also have good activity under another. In this sense, the different experimental conditions can be thought of as noise to whatever the "true" sgRNA activity may be. Thus for analyzing the generalizability of our model, it would be worthwhile to assess how it performs on a noisy dataset.

%\subsubsection{To other Cas enzymes}
%Within the same Deephf study, they also assessed the activity of the sgRNAs with two other Cas enzymes, versions of the wild-type SpCas9 that have been engineered to have better specificity, eSpCas9 \cite{slaymaker2016rationally} and SpCas9-HF \cite{kleinstiver2016high}. These enzymes both work by heavily penalizing mismatches between the sgRNA and target DNA sequence. Since for our dataset we are only using on-target data, we have no mismatches between sgRNA and target. Therefore, we hypothesize that our model should perform similarly on these two other datasets.


\section{Conclusions}
CRISPR/Cas is one of the leading gene editing methodologies and on-target prediction remains a challenging problem. This paper introduces \textit{ChromeCRISPR}, a collection of novel hybrid machine learning models that combines the strengths of CNNs with RNNs and leads to a high efficacy for CRISPR/Cas on-target predictions. The research presented here builds on our previous work. With the addition of deep models, hybridization and adding GC content we were able to outperform other state-of-the-art methods developed from the same dataset (i.e. DeepHF and AttCRISPR).

We achieved this through systematic experiments where we assessed the impact of the depth of the models and adding GC content as a feature. We analyzed how these factors affect the prediction of sgRNA activity values across sgRNAs with various levels of GC content and observed activity. From these analyses, we developed hybrid models that combined the feature extraction capabilities of the CNN with the powerful sequence processing of the RNNs. Our best model, a CNN-GRU hybrid with GC content was able to surpass previous models in terms of both the Spearman correlation and the mean squared error.

The significance of \textit{ChromeCRISPR} lies in its potential to advance CRISPR/Cas-based therapies for genetic disorders and other gene editing experiments. Also, the study presented here presents insights into which machine learning techniques are best suited for this task. Designing highly effective sgRNAs that specifically target the defective DNA can lead to more precise and safe gene editing. This in turn could facilitate the development of treatments for previously incurable genetic diseases. Our study shows the power of hybrid models, as well as the positive impact of adding GC content as a feature. We hope that \textit{ChromeCRISPR} will aid in the establishment of robust predictive models for CRISPR/Cas activity, which in turn could promise a transformative impact on human health and medical advancements.

%\nocite{oreg,schn,pond,smith,marg,hunn,advi,koha,mouse}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}
\section*{Ethics approval and consent to participate}
    Not applicable

\section*{Consent for publication}
    Not applicable

\section*{Availability of data and materials}
\hl{https://github.com/Daneshpajouh/ChromeCRISPR}

\section*{Competing Interests}
  Not applicable

\section*{Funding}
    This project was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) under grant number RGPIN/04971-2020.

\section*{Author's Contributions}
    The experimental design was a joint collaboration between AD and MF. AD implemented and ran the models, while MF analysed the results. KCW provided research supervision and guidance, and reviewed the data and analysis. All authors contributed to the writing and editing of the final manuscript.

\section*{Acknowledgements}
   We would like to acknowledge the Digital Research Alliance of Canada for providing computational resources to run our experiments.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bmc_article}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listofTables is discouraged.
%%
%\section*{Tables}
%\begin{table}[h!]
%\caption{Sample table title. This is where the description of the table %should go.}
%      \begin{tabular}{cccc}
%        \hline
%           & B1  &B2   & B3\\ \hline
%       A1 & 0.1 & 0.2 & 0.3\\
%       A2 & ... & ..  & .\\
%       A3 & ..  & .   & .\\ \hline
%      \end{tabular}
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{Additional Files}
%  \subsection*{Additional file 1 --- Sample additional file title}
%    Additional file descriptions text (including details of how to
%    view the file, if it is in a non-standard format or the file extension).  This might
%    refer to a multi-page table or a figure.
%
%  \subsection*{Additional file 2 --- Sample additional file title}
%    Additional file descriptions text.


\end{backmatter}
\end{document}
